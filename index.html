<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Parameterized Reconstruction</title>
    <link href="prism.css" rel="stylesheet" />
    <style>
      body {
        font-family: system-ui, sans-serif;
        max-width: 900px;
        margin: 2rem auto;
        padding: 1rem;
        line-height: 1.6;
        background-color: #f9f9f9;
        color: #333;
      }

      h1,
      h2,
      h3 {
        line-height: 1.2;
        color: #222;
      }

      a {
        color: #007acc;
        text-decoration: none;
      }

      a:hover {
        text-decoration: underline;
      }

      blockquote {
        margin: 1rem 0;
        padding: 0.5rem 1rem;
        border-left: 4px solid #ccc;
        background-color: #fff;
      }

      code {
        background: #eee;
        padding: 0.2em 0.4em;
        border-radius: 4px;
        font-family: monospace;
      }

      pre {
        background: #eee;
        padding: 1em;
        overflow-x: auto;
        border-radius: 4px;
      }

      img {
        max-width: 100%;
        height: auto;
      }

      footer {
        margin-top: 4rem;
        text-align: center;
        font-size: 0.9em;
        color: #777;
      }

      table {
        table-layout: fixed;
        width: 100%;
        border-collapse: collapse;
      }

      table td,
      table th {
        padding: 8px 8px;
        border: 1px solid black;
        text-align: center;
      }
      .check {
        color: green;
        font-weight: bold;
        text-align: center;
        font-size: 30pt;
      }

      .cross {
        color: red;
        font-weight: bold;
        text-align: center;
        font-size: 30pt;
      }

      .maybe {
        color: orange;
        font-weight: bold;
        text-align: center;
        font-size: 30pt;
      }

      .footnote-ref {
        text-decoration: none;
        font-size: 10pt;
        vertical-align: super;
        color: black;
      }

      .footnote {
        font-size: 0.85em;
        color: #555;
        margin-top: 1em;
      }
    </style>
  </head>
  <body>
    <script src="prism.js"></script>
    <article class="language-py">
      <h1>Single Image Reconstruction with Explicitly Parameterized Latent Spaces</h1>

      <p><em>By Hans Gaensbauer • April 21, 2025</em></p>
      <h2>Introduction</h2>
      <p>
        Single image reconstruction is generally under-constrained, since occlusion and
        the 2D projection implicit in the creation of the image destroys information about
        depth. However, for some applications, prior information about the 3D geometry
        drastically reduces the number of parameters required to completely describe the
        scene. For example, in industrial settings when computer vision is to be used to
        perform measurements (for verification) or localization (for pick and place) on
        parts that have a well understood but unknown shape. In these cases, general
        purpose latent representations are <i>too</i> general to elegantly capture the
        prior geometric information, and classical template matching might require too
        much manual fine-tuning or provide insufficient accuracy for the application [1].
      </p>
      <p>
        This work uses 3D models in
        <a href="https://www.blender.org/">Blender</a> to define a parameterization of the
        3D scene. Blender is a well-known and well-supported 3D modelling and animation
        program that has a lot of features for defining complex constraints on deformable
        custom 3D geometry. As an example, to animate a person walking, an artist using
        Blender will define deformations and transformations that parameterize the complex
        movement of the model's legs as they make a walking motion. Then, a single
        parameter can be driven by the frame number and the model will appear to take
        steps. By replacing Blender's renderer with a differential renderer, I can recover
        these model parameters from an image by performing gradient descent on the model
        parameters and pose through the differentiable renderer similar to the
        reconstruction training approach described in [2], [3].
      </p>
      <h3>Related Work</h3>
      <p>
        There is a significant body of work that uses deformable 3D models (or large
        databases of 3D models) to fit meshes and extract the pose of target objects from
        a single photo [4-11]. The proposed approach has orders of magnitude fewer
        optimization parameters because I am optimizing a small number of user-specified
        parameters rather than the position of every point on the mesh. The handful of
        papers that do use explicitly defined low-dimensional 3D parameterizations are
        application specific and therefore use training/inference techniques that do not
        attempt to completely describe the 3D object [12], [13]. On the other end of the
        spectrum, the proposed project has a lot in common with classical approaches to
        object matching using user-specified models, with the important difference that
        these template-matching algorithms do not leverage a differentiable renderer for
        gradient-descent on the image data [14], [15]. Finally, feature matching
        techniques have been established as the standard approach for 3D template
        matching, but they do not handle deformable/customizable models that would cause
        misalignment of the features between the target and reference images.
      </p>
      <h2>Methods: Mesh Fitting with Differential Rendering</h2>
      <p>
        A different approach that can leverage the tools we have for machine learning is
        to use gradient descent to optimize a mesh by rendering views of that mesh that
        correspond to our ground-truth images with a differentiable renderer. We can use
        the L2 loss between the rendered image of our mesh and the ground truth image, and
        back-propagate through the renderer in order to update the positions of the
        vertices. This approach has been described extensively and is relatively
        straightforward to implement using tools like Pytorch3D. Typically, the mesh is
        stored as a Pytorch tensor, and the 3D position of every vertex of the mesh is
        optimized:
      </p>
      <figure>
        <img
          src="Examples/pytorch_full_mesh_fit.gif"
          alt="Multiview Mesh Fitting from the Pytorch3D example"
          width="300"
          style="display: block; margin: 0 auto"
        />
        <figcaption>
          <b>Figure 1:</b> Multiview mesh fitting using differentiable rendering. This is
          the
          <a href="https://pytorch3d.org/tutorials/fit_textured_mesh"
            >mesh-fitting example provided with Pytorch3D </a
          >, which optimizes a per-vertex distortion in order to minimize the silhouette
          loss for 20 different views. This visualization uses a lit/textured renderer,
          but the optimization uses only the single-channel silhouette.
        </figcaption>
      </figure>
      <p>
        While there is nothing preventing us from parameterizing only a subset of the mesh
        vertices, or from adding constraints to the changes that we allow the optimizer to
        make, in practice this quickly becomes inconvenient because Pytorch is not a 3D
        modelling program. A better workflow is to use a dedicated mesh modelling program
        like Blender to create a parameterized model which is then brought into Pytorch
        and optimized. Blender is a particularly attractive choice because it is very
        tightly integrated with Python anyway, and the Blender Python API makes it easy to
        access the model and the user-defined parameters. Blender supports a variety of
        parametric transformations that give us an intuitive way of defining degrees of
        freedom for our model.
      </p>
      <figure>
        <img
          src="Examples/blender_transforms.gif"
          alt="Example Blender transformations and shape keys"
        />
        <figcaption>
          <b>Figure 2:</b> Example Blender transformations and shape keys. The scale,
          position, and rotation of the monkey are driven together, and a shape key is
          used to deform the monkey's eyebrows.
        </figcaption>
      </figure>
      <p>
        Unfortunately, because Blender is not implemented in PyTorch, none of the mesh
        updates can be differentiated by Pytorch. This is a deeper problem inherent in any
        attempt to couple an external program with deep learning tools, since file IO in
        general breaks the gradient computation. Fortunately, because Blender is so well
        integrated with Python, it is possible to extract each of the transform matrices
        associated with the user-specified mesh transformations. This makes it possible to
        keep the mesh in Pytorch while applying transformations that are defined in
        Blender, and the final mesh in Blender can be updated after the optimization
        process.
      </p>
      <h2>Parameterizations in Blender</h2>
      <p>
        Blender is a free and open source program for 3D modelling, sculpting, VFX,
        simulation, video editing, animation, and a whole host of other features. It is
        extremely well supported, meaning it is very straightforward to learn how to use
        it to create custom parameterized models using online tutorials.
      </p>
      <p>
        In Blender, meshes are edited in "Edit Mode", which gives fine control over
        individual mesh vertices. Afterward, the finished mesh can be further transformed
        and positioned in "Object Mode", which is where the render and scene are set up.
        Blender is an extremely feature-rich modelling program and thus there are very
        many modifiers and transformations that can be applied to a model. For this
        project, I will restrict myself to affine transformations applied in Object Mode
        and Shape Keys applied in Edit Mode, which together are sufficient to capture a
        decent range of possible mesh degrees of freedom.
      </p>
      <h3>Object Transformations and Drivers</h3>
      <p>
        After the vertex positions are computed in Blender, it is possible to apply a
        number of additional transformations to the entire mesh in Object Mode. The entire
        mesh is transformed using a 4x4 matrix in homogeneous coordinates to locate the
        object within the scene.
      </p>
      <p>
        In order to parameterize these transformations, Blender includes "drivers" which
        can be used to parameterize transformations. For general use, drivers allow
        certain object properties to be driven by other parameters in the model, but for
        our use, drivers allow us to flag parts of the transformation matrix that we want
        to optimize. For example, we can specify that we only want to vary the X/Y
        position (and not Z) of the object on some known surface, or constrain the aspect
        ratio while allowing the absolute size to be optimized.
      </p>
      <p>
        While there are a miscellany of ways to communicate a specific parameterization to
        the optimizer in Pytorch, the approach that best aligns with the convention for
        drivers in normal Blender use is to define an empty "control object" that has
        custom parameters for each of the variables we wish to optimize. Then, each of the
        drivers that we define for the objects we want to optimize refer to the properties
        of the control object. This is especially convenient because we need to compute
        the world transformation matrix in Pytorch in order to use it in the optimization.
        We can extract each of the control object parameters and assign them to
        gradient-enabled tensors like this:
      </p>
      <pre><code>control = bpy.data.objects["Control"]
custom_props = {}

if control is not None:
    for key, value in control.items():
        if not key.startswith("_"):
            custom_props[key] = value

#Assign each of the custom properties to custom variables
def initialize_params(custom_props):
    params_assign_string = "global opt_props\n"
    params_assign_string += "opt_props = []\n"
    for prop in custom_props:
        params_assign_string += f"global {prop}\n"
        params_assign_string += f"{prop} = torch.tensor([{custom_props[prop]}], device=device, requires_grad=True)\n"
        params_assign_string += f"opt_props.append({prop})\n"
        
    print(params_assign_string)
    exec(params_assign_string)

initialize_params(custom_props)
</code></pre>
      <p>
        As an example, for a Blender model of a parameterized bolt, the snippet above
        creates (and executes) the following code:
      </p>
      <pre><code>global opt_props
opt_props = []
global head_thickness
head_thickness = torch.tensor([0.5], device=device, requires_grad=True)
opt_props.append(head_thickness)
global body_diameter
body_diameter = torch.tensor([0.78], device=device, requires_grad=True)
opt_props.append(body_diameter)
global l_x
l_x = torch.tensor([1.2], device=device, requires_grad=True)
opt_props.append(l_x)
global l_y
l_y = torch.tensor([0.0], device=device, requires_grad=True)
opt_props.append(l_y)
global head_diameter
head_diameter = torch.tensor([3.0], device=device, requires_grad=True)
opt_props.append(head_diameter)
global body_length
body_length = torch.tensor([2.3], device=device, requires_grad=True)
opt_props.append(body_length)
</code></pre>
      <p>
        The initial values of each of the properties are pulled from the Blender file.
      </p>
      <h3>Shape Keys</h3>
      <p>
        Shape Keys are a Blender feature for creating arbitrary parametric mesh
        distortions. The user defines a default mesh configuration and then uses Edit Mode
        to distort the mesh to a desired "end" position. The shape key defines a linear
        interpolation between the start and end vertex positions for the selected
        vertices, weighted by the "key" parameter. For example, this can be used to make a
        character smile or to scale and move certain parts of the mesh. It is possible to
        define multiple shape keys for the same vertices, in which case the final mesh is
        the weighted sum of each of the transformations.
      </p>
      <p>
        To apply a shape key outside of Blender, we extract the "basis" or starting vertex
        positions and the "key" or ending vertex positions, and then compute the new mesh
        as <code>mesh = basis + key*(key-basis)</code>:
      </p>
      <pre><code> # Apply a shape key externally
  basis = cube.shape_keys.key_blocks["Basis"].data
  basis_verts = torch.tensor(np.array([v.co[:] for v in basis], 
      dtype=np.float32)[np.newaxis,:,:], requires_grad=True, device=device)
  key1 = cube.shape_keys.key_blocks["Wider Top"].data
  key1_verts = torch.tensor(np.array([v.co[:] for v in key1], 
      dtype=np.float32)[np.newaxis,:,:], requires_grad=True, device=device)
  
  #Compute the distorted mesh
  keyval = torch.tensor([0.8], requires_grad=True, device=device)
  verts = basis_verts + keyval * (key1_verts - basis_verts)
  mesh = Meshes(verts=verts, faces=faces_tensor, textures=textures)
</code></pre>
      <p>
        Here, <code>keyval</code> is the gradient-enabled tensor used for optimization.
      </p>
      <h3>Blender Model Setup</h3>
      <p>
        In summary, to create a parameterized model for this reconstruction framework, a
        user defines an "Empty" object called "Control" that serves as a container for all
        of the parameters, which are added as custom properties. Then, the user adds
        drivers or shape keys to the mesh (named "Target") in order to define the specific
        ways in which the mesh can be deformed.
      </p>
      <figure>
        <video width="800" controls>
          <source src="Examples/blender_setup.mp4" type="video/mp4" />
          Your browser does not support the video tag.
        </video>
        <figcaption>
          <b>Figure 3:</b> The Blender file setup for parametric reconstruction. First, I
          create an empty control object and add a custom property called 'length'. Then,
          I add a driver to the default cube that scales the size of the cube along the X
          direction based on the value of this property. Then, I create a shape key to
          adjust the skew of the cube along the Y direction.
        </figcaption>
      </figure>
      <p>
        It is possible to create more complex, composite meshes by combining multiple
        objects, each with their own drivers and shape keys. In this case, each of the
        objects to be optimized are added to a collection named "Target".
      </p>
      <h2>Parameterized Mesh Fitting</h2>
      In order to optimize the mesh using Pytorch, it is not enough to extract the
      parameters and compute the transformations once. Rather, we need to extract the
      entire dependency graph and automatically re-implement it in Pytorch so that it is
      fully differentiable.
      <h3>Building a PyTorch Pipeline from Blender Parameterizations</h3>
      Blender gives us access to the expressions used to evaluate each driver, and it is
      relatively straightforward to compute the 4x4 homogeneous transformation matrix from
      the updated driver values in Pytorch. At a high level, our approach looks like this:
      <ol>
        <li>Extract all of the custom properties from the empty control object</li>
        <li>Extract the list of drivers from the objects in the scene</li>
        <li>
          Automatically build a function that computes the drivers from the control object
          properties in Pytorch
        </li>
        <li>
          Use this function to compute an updated transformation matrix during training
        </li>
      </ol>
      <p>
        In practice, this is complicated by the fact that all of the expressions for the
        different dependencies are strings, which need to be parsed at runtime. To do
        this, we build an update_matrix() function from the drivers/parameters at runtime:
      </p>
      <pre><code>obj = bpy.data.objects["Target"]
drivers = {}
#defaults
drivers["location"] =       [{"exp":"0","vars":{}},
                              {"exp":"0","vars":{}},
                              {"exp":"0","vars":{}}]

drivers["rotation_euler"] = [{"exp":"0","vars":{}},
                              {"exp":"0","vars":{}},
                              {"exp":"0","vars":{}}]

drivers["scale"] =          [{"exp":"1","vars":{}},
                              {"exp":"1","vars":{}},
                              {"exp":"1","vars":{}}]

if obj.animation_data and obj.animation_data.drivers:
    for driver in obj.animation_data.drivers:
        drivers[driver.data_path][driver.array_index]["exp"] = driver.driver.expression
        variables = {}
        for var in driver.driver.variables:
            variables[var.name] = var.targets[0].data_path
        drivers[driver.data_path][driver.array_index]["vars"] = variables
else:
    print("No drivers on this object.")

def get_transform_update_string(drivers):
    vstr = "def update_transform():\n"
    for driver in drivers:
        vstr += f"\t{driver} = torch.zeros(3, device=device)\n"
        for axis in range(3):
            vars = drivers[driver][axis]["vars"]
            for var in vars:
                vstr += f"\t{var} = {vars[var][2:-2]}\n"
            vstr
            exp = drivers[driver][axis]["exp"]
            vstr += f"\t{driver}[{axis}] = {exp}\n"
    vstr += "\treturn build_transform(location, rotation_euler, scale)\n"
    return vstr
</code></pre>
      <p>
        As an example, for a cube with rotation and X axis scale that are both driven by a
        single parameter, this code produces the following output:
      </p>
      <pre><code>def update_transform():
  location = torch.zeros(3, device=device)
  location[0] = 0
  location[1] = 0
  location[2] = 0
  rotation_euler = torch.zeros(3, device=device)
  var = scale1
  rotation_euler[0] = 0.3*var
  rotation_euler[1] = 0
  rotation_euler[2] = 0
  scale = torch.zeros(3, device=device)
  var = scale1
  scale[0] = 2*var
  scale[1] = 1
  scale[2] = 1
  return build_transform(location, rotation_euler, scale)
</code></pre>
      <p>
        In this case, <code>scale1</code> has already been defined as a global,
        gradient-enabled tensor by the call to <code>initialize_params</code>. Finally, we
        need to implement <code>build_transform()</code>, which returns a 3x4 tensor that
        transforms homogeneous vertex positions into non-homogeneous transformed vertex
        positions:
      </p>
      <pre><code>def build_transform(location, rotation, scale):
  cx, cy, cz = torch.cos(rotation)
  sxr, syr, szr = torch.sin(rotation)

  ones = torch.ones((), device=device)
  zeros = torch.zeros((), device=device)

  Rx = torch.stack([
      torch.stack([ones, zeros, zeros], dim=-1),
      torch.stack([zeros, cx, -sxr], dim=-1),
      torch.stack([zeros, sxr, cx], dim=-1)
  ], dim=0)

  Ry = torch.stack([
      torch.stack([cy, zeros, syr], dim=-1),
      torch.stack([zeros, ones, zeros], dim=-1),
      torch.stack([-syr, zeros, cy], dim=-1)
  ], dim=0)

  Rz = torch.stack([
      torch.stack([cz, -szr, zeros], dim=-1),
      torch.stack([szr, cz, zeros], dim=-1),
      torch.stack([zeros, zeros, ones], dim=-1)
  ], dim=0)

  R = Rz @ Ry @ Rx

  S = torch.diag(torch.stack([scale[0], scale[1], scale[2]]))
  RS = R @ S

  loc = location.view(3, 1)
  upper = torch.cat([RS, loc], dim=1)
  
  return upper.to(dtype=torch.float32)
</code></pre>
      <h3>Training</h3>
      <p>
        With all this in place, training is straightforward. In each iteration, we use
        <code>update_transform()</code> to obtain an updated transform matrix from the
        optimization parameters, transform the mesh, render, and compute a loss. Then we
        backpropagate to the optimization parameters.
      </p>
      <pre><code>optimizer = torch.optim.Adam(opt_props, lr=0.02)

num_epochs = 100
epochs_per_save = 5
gif_images = np.zeros((num_epochs//epochs_per_save, image_size, image_size, 3), dtype=np.uint8)

with tqdm(range(num_epochs)) as titer:
    for i in titer:
        optimizer.zero_grad()
        matrix_world = update_transform()

        ones = torch.ones(verts_tensor.shape[0], verts_tensor.shape[1], 1, device=verts_tensor.device)
        nverts = einsum(torch.cat([verts_tensor, ones], dim=-1), matrix_world,'b i v, j v-> b i j')
        mesh = Meshes(verts=nverts, faces=faces_tensor, textures=textures)
        cimage = renderer(mesh)
        #Save the image into a GIF to show the training proces
        if(not i % epochs_per_save):
            gif_images[i//epochs_per_save] = (cimage[...,:3]*255).byte().detach().cpu().numpy()

        loss = torch.sum((cimage - target_image)**2)
 
        loss.backward()
        optimizer.step()
        titer.set_postfix(loss=loss.item(), scale1=opt_props[0].item())
</code></pre>
      <figure>
        <p>
          For the tiny example with a cube parameterized such that the length and x axis
          rotation are both driven by a single parameter, training for 100 iterations
          looks like this:
        </p>
        <img src="Examples/outputs/driver_training_process.gif" width="312" />
        <img src="Examples/outputs/driver_demo_training_loss.png" width="312" />
        <figcaption>
          <b>Figure 4:</b> Optimizing a driver. Starting from a value of 1 for the driver,
          it takes about 80 rendering iterations for the updated mesh to converge to the
          target shape.
        </figcaption>
      </figure>
      <h3>Optimizing Shape Keys</h3>
      <p>
        Optimizing shape keys works exactly the same way. The shape keys are applied
        before the object transformation matrix is applied, and the key parameter is
        included with the other control parameters in the optimizer. A very simple example
        where the top face of a cube is distorted and then optimized back to the perfect
        cube for 100 iterations looks like this:
      </p>
      <figure>
        <img src="Examples/outputs/shapekey_training_process.gif" width="312" />
        <img src="Examples/outputs/shapekey_demo_training_loss.png" width="312" />
        <figcaption>
          <b>Figure 5:</b> Optimizing a shape key. The target shape here is the starting
          default cube, and the starting point has the value of the shape key set to 3.
          Here, the shape key scales the edge nearest to the camera.
        </figcaption>
      </figure>
      <h3>Composite Meshes and Extracting 2D Location</h3>
      <p>
        A single transform matrix is not terribly useful on its own, since a single affine
        transformation is too simple to parameterize an interesting range of possible
        shapes. However, the driver+transform matrix approach is readily extended to more
        complicated Blender files featuring composite meshes. Each mesh in the scene can
        have its own parameterized transformations and shape keys, and the drivers for
        these transformations can depend on properties of other meshes.
      </p>
      <p>
        For example, the model of the bolt shown below consists of two cylinders: one for
        the head and one for the body of the bolt. The height and diameter of each
        cylinder are parameterized, and the body of the bolt is translated so that it
        stays attached to the head as the head height changes. Furthermore, the entire
        bolt has a parameterized XY location, which is intended to capture an unknown
        location on a known ground plane. The photo to the right shows the parameters
        defined in Blender.
      </p>
      <figure>
        <img src="Examples/bolt.PNG" width="312" />
        <img src="Examples/bolt_params.PNG" width="275" />
        <figcaption>
          <b>Figure 6:</b> Screen captures from Blender showing an example composite mesh
          and the custom properties for the Control object. The bolt model shown here
          consists of two cylinders that are individually transformed to generalize the
          shape of a bolt.
        </figcaption>
      </figure>
      <p>
        The function <code>get_transform_update_string()</code> must be updated to produce
        a function that returns a list of transform matrices, one for each mesh in the
        target collection (Blender's way of denoting a composite mesh):
      </p>
      <pre><code>def get_transform_update_string(drivers):
  vstr = "def update_transform():\n"
  vstr += "\ttransform_matrices = []\n\n"
  for drivers in drivers_list:
      for driver in drivers:
          vstr += f"\t{driver} = torch.zeros(3, device=device)\n"
          for axis in range(3):
              vars = drivers[driver][axis]["vars"]
              for var in vars:
                  vstr += f"\t{var} = {vars[var][2:-2]}\n"
              vstr
              exp = drivers[driver][axis]["exp"]
              vstr += f"\t{driver}[{axis}] = {exp}\n"
      vstr += "\ttransform_matrices.append(build_transform(location, rotation_euler, scale))\n\n"
  vstr += "\treturn transform_matrices\n"
  return vstr</code></pre>
      <p>
        The training loop now includes an inner loop that transforms each mesh in the
        scene:
      </p>
      <pre><code>with tqdm(range(num_epochs)) as titer:
  for i in titer:
      optimizer.zero_grad()
      transform_matrices = update_transform()
      meshes = []
      # Apply each of the transforms
      for j in range(len(transform_matrices)):
          matrix_world = transform_matrices[j]
          ones = torch.ones(verts_tensor_list[j].shape[0], verts_tensor_list[j].shape[1], 1, device=device)
          nverts = einsum(torch.cat([verts_tensor_list[j], ones], dim=-1), matrix_world,'b i v, j v-> b i j')
          meshes.append(Meshes(verts=nverts, faces=faces_tensor_list[j], textures=textures_list[j]))
      scene = join_meshes_as_scene(meshes, True)
      
      cimage = renderer(scene, cameras=cameras, lights=lights)[0,...,3]
      #Save the image into a GIF to show the training proces
      if(not i % epochs_per_save):
          simage = visrenderer(scene)
          gif_images[i//epochs_per_save] = (simage[...,:3]*255).byte().detach().cpu().numpy()

      loss = torch.sum((cimage - target_image)**2)

      loss.backward()
      optimizer.step()
      titer.set_postfix(loss=loss.item(), length=opt_props[-1].item())
</code></pre>
      <p>
        This code can accept any number of objects in the composite mesh. Training for 300
        iterations gives the following results:
      </p>
      <figure>
        <img src="Examples/outputs/location_training_process.gif" width="312" />
        <img src="Examples/outputs/bolt_demo_training_loss.png" width="312" />
        <figcaption>
          <b>Figure 7:</b> Training animation and loss for composite mesh fitting,
          including 2D location.
        </figcaption>
      </figure>
      <p>
        One significant challenge with fitting the XY location occurs when there is
        insufficient overlap between the starting mesh and the target image. In these
        cases, MSE loss does not produce enough of a gradient that points towards the
        correct location since the location does not significantly affect the loss until
        the mesh and target object are close enough that the "fuzziness" from the
        differentiable renderer has a significant amount of overlap. In these cases, the
        model will sometimes "cheat" by attempting to shrink the target mesh to reduce the
        error in the pixels that the target mesh is occupying:
      </p>
      <figure>
        <img
          src="Examples/outputs/location_training_process_translate_og.gif"
          width="312"
        />
        <img
          src="Examples/outputs/bolt_demo_training_loss_translate_og.png"
          width="312"
        />
        <figcaption>
          <b>Figure 8:</b> Training can fail when there is insufficient overlap between
          the target silhouette and the rendering of the starting mesh because the
          optimizer learns to "crush" the mesh to a point. The loss converges to the error
          between an empty scene and the target rendering.
        </figcaption>
      </figure>
      <p>
        Notice that the loss "converges" to a large nonzero value. One possible fix is to
        blur the images so there is always some overlap. Anecdotally, doing this through
        the differential renderer produced strange side effects, so instead I implemented
        it using kornia.gaussian_blur2d with a standard deviation of 10 and a kernel size
        of (101,101). The new target image looks like this:
      </p>
      <figure>
        <img src="Examples/outputs/bolt_blur.png" width="312" />
        <figcaption>
          <b>Figure 9:</b> The target and starting mesh are blurred with a wide kernel to
          incentivise the optimizer to align the mesh and the target silhouette.
        </figcaption>
      </figure>
      <p>
        The results for the same mesh fitting problem and hyperparameters are much better:
      </p>
      <figure>
        <img src="Examples/outputs/location_training_process_translate.gif" width="312" />
        <img src="Examples/outputs/bolt_demo_training_loss_translate.png" width="312" />
        <figcaption>
          <b>Figure 10:</b> Optimization with Gaussian filtered silhouettes. The same
          initial conditions used in Figure 8 result in correct convergence when the
          silhouettes are blurred. Furthermore, training proceeds slightly more quickly,
          possibly because the blurred silhouettes produce better gradients for aligning
          the silhouettes even once the silhouettes have some overlap.
        </figcaption>
      </figure>
      <p>
        Of course, this is not without tradeoffs. The final fit suffers from the loss of
        detail from the blur: this results in the bolt head being slightly too thin even
        though the loss converges. Additionally, while the blur causes training to
        converge in slightly fewer iterations, it triples (3m21s vs 1m04s) the amount of
        time it takes to train because the blur kernel must be very large to get some
        overlap when the objects start far away. It may be possible to dynamically adjust
        the level of blur during training to get the best of both worlds but this is
        outside the scope of this project.
      </p>
      <h2>Fitting Photos</h2>
      <p>
        For the proposed technique to be useful, it must be able to handle real photos as
        target images, rather than "perfect" renderings created in Pytorch3D. The
        optimizer uses silhouette loss, so the input images must be preprocessed by
        reducing them to a single channel. For this demonstration, I have further reduced
        the image to a 1-bit mask to remove the effects of lighting/shadows.
      </p>
      <figure>
        <img src="Examples/bolt_image.png" width="312" />
        <img src="Examples/outputs/bolt_photo_mask.png" width="312" />
        <figcaption>
          <b>Figure 11:</b> A photo of a bolt and the extracted silhouette used for mesh
          fitting.
        </figcaption>
      </figure>
      <p>
        Since this photo was taken at a different angle than the angle of the camera in
        the renderer, it is not possible to get a perfect match. However, the training
        does still approach the correct geometry:
      </p>
      <figure>
        <img src="Examples/outputs/photo_training_process.gif" width="312" />
        <img src="Examples/outputs/photo_demo_training_loss_badangle.png" width="312" />
        <figcaption>
          <b>Figure 12:</b> "Naive" mesh fitting on the photo in Figure 11. Because the
          photo was taken with an unknown camera position, the ground plane (and thus, the
          central axis of the bolt) are not aligned and a perfect fit is not possible.
          Note that while the original photo is shown for clarity, only the silhouette is
          used for training.
        </figcaption>
      </figure>
      <p>
        For many cases (like pick and place machines), the camera parameters are known
        ahead of time, and this won't be a problem. For cases like this photo where the
        camera position is not known, we can also optimize the camera roll and elevation
        in our training loop:
      </p>
      <pre><code>R, T = look_at_view_transform(dist=14, elev=elevation, azim=0, device=device)
cos = torch.cos(roll/100)
sin = torch.sin(roll/100)

ones = torch.ones((1), device=device)
zeros = torch.zeros((1), device=device)

R_roll = torch.zeros(1,3,3,device=device)
R_roll[0,0,0] = cos
R_roll[0,0,2] = -sin
R_roll[0,2,0] = sin
R_roll[0,2,2] = cos
R_roll[0,1,1] = 1

R_new = R_roll @ R

cimage = renderer(scene, R=R_new, T=T)[0,...,3]
loss = torch.sum((cimage - target_image)**2)</code></pre>
      <p>
        Here roll and elevation are tensors with <code>requires_grad=True</code>. It's
        worth noting that we cannot also optimize the depth, since there is no scale
        information in the photo. Training for 100 iterations:
      </p>
      <figure>
        <img src="Examples/outputs/photo_training_process_camera.gif" width="312" />
        <img src="Examples/outputs/photo_demo_training_loss_camera.png" width="312" />
        <figcaption>
          <b>Figure 13:</b> The same mesh fitting problem where the camera roll and
          elevation are also optimized. The learning rate has been increased to 0.12.
        </figcaption>
      </figure>
      <p>
        The initial dimensions are the same as in the experiment above - it's hard to see
        the initial state since it changes very quickly at the start of training.
      </p>
      <h2>Comparison to Existing Techniques</h2>
      <p>
        Among the various techniques studied here, this approach is the only one that can
        simultaneously handle variable/deformable geometry <i>and</i> single images. The
        catch is that this is only because it "cheats" with the prior information about
        the scene encoded in the parameterization. Most of the effort required to get this
        approach working (and much of what makes it valuable and novel) is the integration
        with Blender, which converts the normally arduous and subtle task of constraining
        the optimization process into the extensively documented and relatively convenient
        process of building a parametric model in widely used and free modelling software.
      </p>
      <p>
        Compared to generalized mesh fitting approaches, this technique has far fewer free
        variables and thus can succeed in correctly fitting a mesh to otherwise
        under-constrained situations like single images. While general single-image
        reconstruction approaches rely on a training dataset and neural networks to learn
        an acceptable parameterization [2], this approach skips that step in favor of an
        explicitly defined latent space. For applications where the parameterization is
        not obvious but training data is available, learned representations are a better
        choice. In situations where the the parameterization is obvious, it makes more
        sense to provide it to the optimizer as described here.
      </p>
      <p>
        The "template matching" aspect of this project can be done more quickly using SIFT
        feature matching, which does not require the rendering step, however, these
        techniques cannot handle deformable or otherwise parameterized meshes. Because
        backpropagation and rendering for full mesh optimization is highly parallel, the
        speedup associated with the smaller set of free variables isn't as significant as
        I had expected. Judging from the results of these experiments on a GTX 1650 GPU
        (80ms rendering time), I expect a carefully tuned version of this approach with
        better hardware could be "near real time" like SIFT [16], but it's not there in
        its current state.
      </p>
      <figure>
        <table>
          <tr>
            <th></th>
            <th>Single Image Reconstruction</th>
            <th>Feature Matching</th>
            <th>Mesh Fitting</th>
            <th>Parameterized Reconstruction</th>
          </tr>
          <tr>
            <th>Works with Single Images</th>
            <td class="check">✔</td>
            <td class="check">✔</td>
            <td class="cross">✖</td>
            <td class="check">✔</td>
          </tr>
          <tr>
            <th>Works with Photos</th>
            <td class="maybe">
              ~<sup><a href="#footnote1" class="footnote-ref">1</a></sup>
            </td>
            <td class="check">✔</td>
            <td class="check">✔</td>
            <td class="check">✔</td>
          </tr>
          <tr>
            <th>Extract 2D Position</th>
            <td class="check">✔</td>
            <td class="check">✔</td>
            <td class="check">✔</td>
            <td class="check">✔</td>
          </tr>
          <tr>
            <th>Generalizes to New Shapes</th>
            <td class="cross">✖</td>
            <td class="cross">✖</td>
            <td class="check">✔</td>
            <td class="check">✔</td>
          </tr>
          <tr>
            <th>Degree of Customization Required</th>
            <td>High (Custom Dataset [2-3,6,17])</td>
            <td>Very Low</td>
            <td>None [16]</td>
            <td>Some (Custom Model)</td>
          </tr>
          <tr>
            <th>Speed</th>
            <td>
              Near Real-Time<sup><a href="#footnote2" class="footnote-ref">2</a></sup>
            </td>
            <td>Near Real-Time [18]</td>
            <td>Not Real Time</td>
            <td>Not Real Time</td>
          </tr>
        </table>
        <p id="footnote1" class="footnote">1. Sometimes - see [2] and [17]</p>
        <p id="footnote2" class="footnote">
          2. Scales quadratically with image resolution [17]
        </p>
        <figcaption>
          <b>Figure 14: </b> A comparison of several different techniques for 3D template
          matching and reconstruction.
        </figcaption>
      </figure>

      <h2>Conclusion and Limitations</h2>
      <p>
        This technique can fit parametric, composite models from Blender to single photos
        of real objects in about 100 iterations. All of the experiments described here
        were run on a single GTX 1650 GPU, which is a low-cost ($100 on Ebay) GPU that is
        not competitive with GPUs used for research today, so it is likely that the
        rendering times I had on the GTX 1650 are longer than they would be on a modern
        GPU. Furthermore, time constraints and the limited availability of the (borrowed)
        compute resources meant that the training hyperparameters are probably not
        optimal, and I was only able to set up a few mesh fitting problems. It would be
        nice to try this technique on more models to get a more general picture of its
        performance across a wider variety of setups.
      </p>
      <p>
        In addition to these "real-world" considerations, there are a few inherent
        limitations to this technique as presented. First, while object transforms,
        drivers, and shape keys capture a wide variety of Blender parameterizations, they
        don't come close to covering the whole range of possible constraints that Blender
        allows. Because every Blender feature I include has to be dynamically rebuilt in
        Pytorch to be differentiable, the code to handle it in the optimization has to be
        developed manually. In my opinion, the biggest missing feature in the current
        presentation is
        <a
          href="https://docs.blender.org/manual/en/latest/animation/introduction.html#rigging"
          >rigging</a
        >, which is used to define flexible joints for posing characters in 3D animation.
        A rigging implementation in Pytorch would allow the differentiable rendering
        pipeline to be used to extract 3D pose from photos of figures - a very interesting
        problem with a wide range of real-world applications.
      </p>
      <p>
        Furthermore, another limitation of this approach is that it does not significantly
        reduce the amount of computation required compared to other techniques with much
        larger numbers of free parameters. The gradient descent step requires hundreds of
        iterations, each requiring a call to the renderer. On a GTX 1650 GPU, this can
        take several minutes, and this is too slow for real-time applications working with
        video. Real-time application of this technique may still be possible with a better
        GPU and more fine-tuning. Practically, the most significant impact of the low
        dimensionality of the latent space is that it is possible to reliably and
        accurately fit the 3D mesh to a single 2D image, which is not possible with
        general mesh fitting techniques. It is possible that it is faster to uniformly
        sample the entire parameter space and pick the model with the lowest loss
        (skipping the backpropagation step altogether), especially for models with only a
        few parameters like the examples shown here. This would have the rather important
        advantage that Blender's built-in renderer (and all of Blender's other features)
        could be used. This ultimately depends on how much of the iteration time is spend
        on rendering, and how much is spent on backpropagation: for the bolt example with
        the real image, rendering took 78ms on average, and backpropagation took 28ms on
        average.
      </p>
      <p>
        I claim that this pipeline (or a version of it) is the best option for a multitude
        of industrial vision problems where the range of variability in the target objects
        is well understood ahead of time. For example, measuring the height of electrical
        components with a known footprint, identifying ice cream sandwiches with too much
        (or too little!) filling, or sorting recyclable bottles based on their shape or
        appearance.
      </p>
      <figure>
        <img src="Examples/cap.gif" , width="170" />
        <img src="Examples/sandwich.gif" , width="400" />
        <img src="Examples/can.gif" , width="170" />
        <figcaption><b>Figure 15:</b> A few possible applications.</figcaption>
      </figure>
      <h2>Acknowledgements</h2>
      <p>
        I would like to thank <a href="https://github.com/jamelofs">James Elofson</a> for
        making his server and GPU available to run the experiments in this project.
      </p>
      <h2>Code</h2>
      All of the code is available on
      <a href="https://github.com/hansgaensbauer/BlenderModelFitting">Github</a>.
      <h2>References</h2>
      <ol>
        <li>
          D. Shin, C. C. Fowlkes, and D. Hoiem, “Pixels, Voxels, and Views: A Study of
          Shape Representations for Single View 3D Object Shape Prediction,” in
          <i>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, Salt
          Lake City, UT: IEEE, Jun. 2018, pp. 3061–3069. doi:
          <a href="https://doi.org/10.1109/CVPR.2018.00323">10.1109/CVPR.2018.00323</a>.
        </li>
        <li>
          S. Popov, P. Bauszat, and V. Ferrari, “CoReNet: Coherent 3D scene reconstruction
          from a single RGB image,” Aug. 05, 2020, <i>arXiv</i>: arXiv:2004.12989. doi:
          <a href="https://doi.org/10.48550/arXiv.2004.12989">10.48550/arXiv.2004.12989</a
          >.
        </li>
        <li>
          P. Henderson and V. Ferrari, “Learning single-image 3D reconstruction by
          generative modelling of shape, pose and shading,” Aug. 26, 2019,
          <i>arXiv</i>: arXiv:1901.06447. doi:
          <a href="https://doi.org/10.48550/arXiv.1901.06447">10.48550/arXiv.1901.06447</a
          >.
        </li>
        <li>
          A. Chan, “Adventures with Differentiable Mesh Rendering,”
          <i>Distill</i>. Accessed: Apr. 08, 2025. [Online]. Available:
          <a href="http://andrewkchan.dev/">http://andrewkchan.dev/</a>
        </li>
        <li>
          N. Kholgade, T. Simon, A. Efros, and Y. Sheikh, “3D object manipulation in a
          single photograph using stock 3D models,”
          <i>ACM Trans. Graph.</i>, vol. 33, no. 4, pp. 1–12, Jul. 2014. doi:
          <a href="https://doi.org/10.1145/2601097.2601209">10.1145/2601097.2601209</a>.
        </li>
        <li>
          J. J. Lim, A. Khosla, and A. Torralba, “FPM: Fine Pose Parts-Based Model with 3D
          CAD Models,” in
          <i>Computer Vision – ECCV 2014</i>, vol. 8694, D. Fleet, T. Pajdla, B. Schiele,
          and T. Tuytelaars, Eds., Lecture Notes in Computer Science, Cham: Springer,
          2014, pp. 478–493. doi:
          <a href="https://doi.org/10.1007/978-3-319-10599-4_31"
            >10.1007/978-3-319-10599-4_31</a
          >.
        </li>
        <li>
          K. Rematas, T. Ritschel, M. Fritz, and T. Tuytelaars, “Image-Based Synthesis and
          Re-synthesis of Viewpoints Guided by 3D Models,” in
          <i>2014 IEEE Conference on Computer Vision and Pattern Recognition</i>,
          Columbus, OH, USA: IEEE, Jun. 2014, pp. 3898–3905. doi:
          <a href="https://doi.org/10.1109/CVPR.2014.498">10.1109/CVPR.2014.498</a>.
        </li>
        <li>
          M. Aubry, D. Maturana, A. A. Efros, B. C. Russell, and J. Sivic, “Seeing 3D
          Chairs: Exemplar Part-Based 2D-3D Alignment Using a Large Dataset of CAD
          Models,” in
          <i>2014 IEEE Conference on Computer Vision and Pattern Recognition</i>, Jun.
          2014, pp. 3762–3769. doi:
          <a href="https://doi.org/10.1109/CVPR.2014.487">10.1109/CVPR.2014.487</a>.
        </li>
        <li>
          C. B. Choy, M. Stark, S. Corbett-Davies, and S. Savarese, “Enriching object
          detection with 2D-3D registration and continuous viewpoint estimation,” in
          <i>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>,
          Boston, MA, USA: IEEE, Jun. 2015, pp. 2512–2520. doi:
          <a href="https://doi.org/10.1109/CVPR.2015.7298866">10.1109/CVPR.2015.7298866</a
          >.
        </li>
        <li>
          J. Rock, T. Gupta, J. Thorsen, J. Gwak, D. Shin, and D. Hoiem, “Completing 3D
          object shape from one depth image,” in
          <i>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>,
          Boston, MA, USA: IEEE, Jun. 2015, pp. 2484–2493. doi:
          <a href="https://doi.org/10.1109/CVPR.2015.7298863">10.1109/CVPR.2015.7298863</a
          >.
        </li>
        <li>
          M. Yavartanoo, J. Chung, R. Neshatavar, and K. M. Lee, “3DIAS: 3D Shape
          Reconstruction with Implicit Algebraic Surfaces,” Aug. 19, 2021,
          <i>arXiv</i>: arXiv:2108.08653. doi:
          <a href="https://doi.org/10.48550/arXiv.2108.08653">10.48550/arXiv.2108.08653</a
          >.
        </li>
        <li>
          K. Genova, F. Cole, A. Maschinot, A. Sarna, D. Vlasic, and W. T. Freeman,
          “Unsupervised Training for 3D Morphable Model Regression,” in
          <i>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, Salt
          Lake City, UT, USA: IEEE, Jun. 2018, pp. 8377–8386. doi:
          <a href="https://doi.org/10.1109/CVPR.2018.00874">10.1109/CVPR.2018.00874</a>.
        </li>
        <li>
          M. Z. Zia, M. Stark, B. Schiele, and K. Schindler, “Detailed 3D Representations
          for Object Recognition and Modeling,”
          <i>IEEE Trans. Pattern Anal. Mach. Intell.</i>, vol. 35, no. 11, pp. 2608–2623,
          Nov. 2013. doi:
          <a href="https://doi.org/10.1109/TPAMI.2013.87">10.1109/TPAMI.2013.87</a>.
        </li>
        <li>
          D. Roller, K. Daniilidis, and H. H. Nagel, “Model-based object tracking in
          monocular image sequences of road traffic scenes,”
          <i>Int. J. Comput. Vis.</i>, vol. 10, no. 3, pp. 257–281, Jun. 1993. doi:
          <a href="https://doi.org/10.1007/BF01539538">10.1007/BF01539538</a>.
        </li>
        <li>
          L. G. Roberts, “Machine perception of three-dimensional solids,” Thesis,
          Massachusetts Institute of Technology, 1963. Accessed: Apr. 08, 2025. [Online].
          Available:
          <a href="https://dspace.mit.edu/handle/1721.1/11589"
            >https://dspace.mit.edu/handle/1721.1/11589</a
          >
        </li>

        <li>
          D. G. Lowe, “Distinctive Image Features from Scale-Invariant Keypoints,”
          International Journal of Computer Vision, vol. 60, no. 2, pp. 91–110, Nov. 2004,
          doi:
          <a href="10.1023/B:VISI.0000029664.99615.94"
            >10.1023/B:VISI.0000029664.99615.94</a
          >.
        </li>

        <li>
          V. Sitzmann, M. Zollhoefer, and G. Wetzstein, “Scene Representation Networks:
          Continuous 3D-Structure-Aware Neural Scene Representations,” in Advances in
          Neural Information Processing Systems, Curran Associates, Inc., 2019. Accessed:
          May 03, 2025. [Online]. Available:
          <a
            href="https://papers.nips.cc/paper_files/paper/2019/hash/b5dc4e5d9b495d0196f61d45b26ef33e-Abstract.html"
          >
            https://dspace.mit.edu/handle/1721.1/11589 </a
          >.
        </li>
        <li>
          M. Kennerley, “A Comparison of SIFT, SURF and ORB on OpenCV,” Medium. Accessed:
          May 03, 2025. [Online]. Available:
          <a
            href="https://mikhail-kennerley.medium.com/a-comparison-of-sift-surf-and-orb-on-opencv-59119b9ec3d0"
          >
            https://mikhail-kennerley.medium.com/a-comparison-of-sift-surf-and-orb-on-opencv-59119b9ec3d0 </a
          >.
        </li>
      </ol>
    </article>
  </body>
</html>
